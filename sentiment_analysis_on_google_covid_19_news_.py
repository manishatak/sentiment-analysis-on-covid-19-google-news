# -*- coding: utf-8 -*-
"""Sentiment Analysis on Google Covid-19 news .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18o5taW0awXKDMHKJ0xgD8U44eJTUAa4i

### **Sentimant Analysis on Google News about Covid-19**

---

### **Load Dataset**

Install live data from Google news API
"""

pip install GoogleNews

"""Import necessary libraries"""

import pandas as pd
import nltk
import matplotlib.pyplot as plt
import spacy
import datetime as dt

nltk.download('all')

"""Load Dataset"""

from GoogleNews import GoogleNews
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
from nltk.collections import Counter
pos_spacy = spacy.load("en_core_web_sm")
nltk.download('vader_lexicon')

"""Define a keyword to search in the Google news Api"""

keyword = 'Covid-19'

"""Perform news scraping from Google and extract the result into Pandas dataframe."""

googlenews = GoogleNews(lang='en', region='IN', period='20d', encode='utf-8')
googlenews.clear()
googlenews.search(keyword)
googlenews.get_page(15)
news_result = googlenews.result(sort=True)
news_data_df = pd.DataFrame.from_dict(news_result)

"""Export dataframe to csv file"""

news_data_df.to_csv('Covid-19_google_news_sentiment_analysis.csv', index=False, encoding="utf-8-sig")

"""### **Data Preprocessing**

Information on DataFrame
"""

news_data_df.info()

"""Checking null values"""

news_data_df.isnull().sum()

"""Filling missing Values"""

news_data_df.fillna(value = "", inplace = True)

news_data_df.head()

"""Performing tokenization, lemmatization, removing stop words and removing HTML tag"""

def preprocess_text(text):

    # Tokenize the text
    tokens = word_tokenize(text.lower())

    # Removing HTML Tags
    soup = BeautifulSoup(text, 'lxml')
    return soup.get_text()

    # Remove stop words
    filtered_tokens = [token for token in soup if token not in stopwords.words('english')]

    # Lemmatize the tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

    # Join the tokens back into a string
    processed_text = ' '.join(lemmatized_tokens)
    return processed_text

news_data_df['desc'] = news_data_df['desc'].apply(preprocess_text)
news_data_df['desc']

"""Selecting adjectives from the news description and appending them to our dataframe

### **Sentiment Analysis**
"""

def percentage(part,whole):
    return 100 * float(part)/float(whole)

#Assigning Initial Values
positive = 0
negative = 0
neutral = 0
#Creating empty lists
news_list = []
neutral_list = []
negative_list = []
positive_list = []
df_new = []
senti_value = []

#Iterating over the news in the dataframe
for news in news_data_df['desc']:
    news_list.append(news)
    analyzer = SentimentIntensityAnalyzer().polarity_scores(news)

    neg = analyzer['neg']
    df_new.append(neg)
    senti_value.append("-1")

    neu = analyzer['neu']
    df_new.append(neu)
    senti_value.append("0")

    pos = analyzer['pos']
    df_new.append(pos)
    senti_value.append("+1")

    comp = analyzer['compound']   
    news_data_df['weigh_of_sentiment'] = pd.Series(df_new)
    news_data_df['sentiment'] = pd.Series(senti_value)

    if neg > pos:
        negative_list.append(news) #appending the news that satisfies this condition
        negative += 1 #increasing the count by 1
    elif pos > neg:
        positive_list.append(news) #appending the news that satisfies this condition
        positive += 1 #increasing the count by 1
    elif pos == neg:
        neutral_list.append(news) #appending the news that satisfies this condition
        neutral += 1 #increasing the count by 1 

positive = percentage(positive, len(news_data_df)) #percentage is the function defined above
negative = percentage(negative, len(news_data_df))
neutral = percentage(neutral, len(news_data_df))

#Converting lists to pandas dataframe
news_list = pd.DataFrame(news_list)
neutral_list = pd.DataFrame(neutral_list)
negative_list = pd.DataFrame(negative_list)
positive_list = pd.DataFrame(positive_list)
#using len(length) function for counting
print("Positive Sentiment:", '%.2f' % len(positive_list), end='\n')
print("Neutral Sentiment:", '%.2f' % len(neutral_list), end='\n')
print("Negative Sentiment:", '%.2f' % len(negative_list), end='\n')

news_data_df.head()

"""### **Data Visualization**

Visualize news description sentiment by pie chart
"""

labels = ['Positive ['+str(round(positive))+'%]' , 'Neutral ['+str(round(neutral))+'%]','Negative ['+str(round(negative))+'%]']
sizes = [positive, neutral, negative]
colors = ['green', 'blue','red']
patches, texts = plt.pie(sizes,colors=colors, startangle=90)
plt.style.use('default')
plt.legend(labels)
plt.title("Sentiment Analysis Result for COVID-19")
plt.axis('equal')
plt.show()

"""### **Looking at the most common adjectives**

Selecting adjectives from the news description and appending them to dataframe
"""

pos_adj=[]
for token in news_data_df['desc']:
    desc = pos_spacy(token)
    pos_token = [tok.lemma_.lower().strip() for tok in desc if tok.pos_ == 'ADJ']
    pos_adj.append(pos_token)

news_data_df['pos_adj'] = pd.Series(pos_adj)

news_data_df.head()

"""Looking at most common positive adjectives used in News description"""

positive_adj = news_data_df[news_data_df['sentiment']=="+1"]['pos_adj']
pos_words = [token for token in positive_adj for token in set(token)]
pos_adj_counter = Counter(pos_words). most_common(15)
print(pos_adj_counter)

"""Looking at most common negative adjectives used in News description"""

negative_adj = news_data_df[news_data_df['sentiment']=="-1"]['pos_adj']
pos_words = [token for token in negative_adj for token in set(token)]
pos_adj_counter = Counter(pos_words). most_common(15)
print(pos_adj_counter)

"""Looking at most common neutral adjectives used in News description"""

neutral_adj = news_data_df[news_data_df['sentiment']=="0"]['pos_adj']
pos_words = [token for token in neutral_adj for token in set(token)]
pos_adj_counter = Counter(pos_words). most_common(15)
print(pos_adj_counter)